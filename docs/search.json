[
  {
    "objectID": "Project2_SDS192/WCA_analysis.html",
    "href": "Project2_SDS192/WCA_analysis.html",
    "title": "SDS 192 Project 2",
    "section": "",
    "text": "Code\n## Loading all the data files\n## DO NOT CHANGE THIS CODE\nchampionships &lt;- read_tsv(\"WCA_data/WCA_export_championships.tsv\")\ncompetitions &lt;- read_tsv(\"WCA_data/WCA_export_Competitions.tsv\")\ncontinents &lt;- read_tsv(\"WCA_data/WCA_export_Continents.tsv\")\ncountries &lt;- read_tsv(\"WCA_data/WCA_export_Countries.tsv\")\neligible_country_iso2s_for_championship &lt;- read_tsv(\"WCA_data/WCA_export_eligible_country_iso2s_for_championship.tsv\")\nevents &lt;- read_tsv(\"WCA_data/WCA_export_Events.tsv\")\nformats &lt;- read_tsv(\"WCA_data/WCA_export_Formats.tsv\")\npersons &lt;- read_tsv(\"WCA_data/WCA_export_Persons.tsv\")\nranksaverage &lt;- read_tsv(\"WCA_data/WCA_export_RanksAverage_333.tsv\")\nrankssingle &lt;- read_tsv(\"WCA_data/WCA_export_RanksSingle_333.tsv\")\nresults &lt;- read_tsv(\"WCA_data/WCA_export_Results_333.tsv\")\nroundtypes &lt;- read_tsv(\"WCA_data/WCA_export_RoundTypes.tsv\")\nscrambles &lt;- read_tsv(\"extra_data/WCA_export_Scrambles_333.tsv\")\n\n\nProject Question: How did people’s first round times in the Munich Open in 2019 compare to their first round times in the Euro competition in 2022?\n\n\nCode\n#People who competed in the Munich open in 2019 and people who competed in Euro competition in 2022\n#Filter observations of people who participated in the 1st round of the Munich & Euro open\nmunich2019 &lt;- results |&gt;\n  select(competitionId, best, average, personId, roundTypeId) |&gt;\n  filter(competitionId == \"MunichOpen2019\") |&gt;\n  filter(roundTypeId == \"1\") \n\neuro2022 &lt;- results |&gt;\n  select(competitionId, best, average, personId, roundTypeId) |&gt;\n  filter(competitionId == \"Euro2022\") |&gt;\n  filter(roundTypeId == \"1\") \n\n\n\n\nCode\n#people who competed in both competitions\nboth_comps &lt;- inner_join(x = munich2019, \n           y = euro2022,\n           by = c(\"personId\" = \"personId\"),\n           suffix = c(\".2019\", \".2022\"))\n\n\n\nSummarization Tables\n\n\nCode\n#both_comps table\nsummarise(both_comps, avg_2019 = mean(average.2019)) |&gt;\n  kable(caption = \"Average Time Munich 2019\")\n\n\n\nAverage Time Munich 2019\n\n\navg_2019\n\n\n\n\n1191.3\n\n\n\n\n\nCode\nsummarise(both_comps, avg_2022 = mean(average.2022)) |&gt;\n  kable(caption = \"Average time Euros 2022\")\n\n\n\nAverage time Euros 2022\n\n\navg_2022\n\n\n\n\n1088.8\n\n\n\n\n\nCode\nsummarise(both_comps, best_2019 = mean(best.2019)) |&gt;\n  kable(caption = \"Best time Munich 2019\")\n\n\n\nBest time Munich 2019\n\n\nbest_2019\n\n\n\n\n1035.5\n\n\n\n\n\nCode\nsummarise(both_comps, best_2022 = mean(best.2022)) |&gt;\n  kable(caption = \"Best time Euros 2022\")\n\n\n\nBest time Euros 2022\n\n\nbest_2022\n\n\n\n\n940.2\n\n\n\n\n\n\n\nVisualization\n\n\nCode\n#both_comps plot (visualization)\nggplot(both_comps, aes(x = personId)) +\n  geom_point(aes(y = average.2019, color = \"2019\"), position = position_dodge(width = 0.3), size = 2, alpha = 0.8) +\n  geom_point(aes(y = average.2022, color = \"2022\"), position = position_dodge(width = 0.3), size = 2, alpha = 0.8) +\n  geom_segment(aes(x = factor(personId), xend = factor(personId), y = average.2019, yend = average.2022), color = \"gray\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n  labs(title = \"Comparison of Average First Round Times From 2019 to 2022\",\n       subtitle = \"Competitors in both Munich Open 2019 and Euro Open 2022\",\n       x = \"Person ID\",\n       y = \"Average Time\",\n       color = \"Year\")\n\n\n\n\n\n\n\n\n\n\n\nBlog Post\nThe data set comes from data recorded at competitions hosted by the World Cube Association which are Rubix cube competitions held around the world. At these competitions, people compete to solve a Rubix cube as fast as they can. They get five attempts. Each person’s middle three times, their average (of their middle three times), and their best time are recorded in the data set among many other variables like each individual’s home country, their gender, name, player ID and the venue where the competition was held. When looking at the data set, we decided we wanted to look at how a player’s times improved over a certain period of time. The Munich Open and the Euros 2022 particularly interested us because the time between them was during the COVID-19 pandemic so we thought there could be noteworthy differences in individuals’ times. The question we wanted to answer was: how did people’s first round times in the Munich Open in 2019 compare to their first round times in the Euro competition in 2022?\nTo answer this, we created a table from the original table that only had the people who competed in both the Munich 2019 and Euro 2022 competitions. We also only looked at the first round times for each competitor. With that new set of data, we made tables that had the average time in each competition and the best time in each competition. From that, we found that among the people who competed in both the Munich Open and the Euros, on average, their average first round time dropped by 102.5 seconds from 2019 to 2022. Also, the best first round time dropped by 95.3 seconds from Munich in 2019 to the Euros in 2022. To further answer our question, we made a point plot of the average times in 2019 and the average times in 2022 for each individual. Looking at the plot, you can see that of the 20 people who participated in both competitions, 14 of them improved their time.\nThe analysis shows that on average, competitors who participated in both the Munich Open 2019 and the Euros 2022 improved their first-round times significantly. The average first-round time dropped by 102.5 seconds, and the best first-round time improved by 95.3 seconds. Additionally, 14 out of 20 competitors showed improvement. So in the three years in between the two competitions, players significantly improved their skills. An ethical concern with this data is the privacy of the data and whether or not the competitors consented to having their information (name, player ID and results) posted publicly."
  },
  {
    "objectID": "Project2_SDS192/extra_data/readme.html",
    "href": "Project2_SDS192/extra_data/readme.html",
    "title": "Ayano Tamura",
    "section": "",
    "text": "This just has one extra file showing the possible scrambles of the rubix cube."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Hi there! Welcome to my website. Feel free to navigate through the About page to learn more about me, and each of the projects I have completed this semester in my SDS192 class.\n\n\n\nA visualization I made for a project"
  },
  {
    "objectID": "Project3_SDS192/mapping.html",
    "href": "Project3_SDS192/mapping.html",
    "title": "SDS 192 Project 3",
    "section": "",
    "text": "Visualization Maps\n\n\nCode\n# If you need to read in csv data do it here. \n\ntornadoes_data &lt;- read.csv(\"tornados.csv\")\n\n\n\n\nCode\n# Group member 1 work here!\nusa &lt;- map_data(\"state\")\n\nplot_tornadoes_year &lt;- function(year_input) {\n  # Filter tornadoes for the given year\n  tornadoes_year &lt;- tornadoes_data |&gt;\n    filter(yr == year_input, !is.na(mag))\n  \n  # Make the map\n  ggplot() +\n    geom_polygon(data = usa, aes(x = long, y = lat, group = group), \n                 fill = \"white\", color = \"black\") +\n    geom_point(data = tornadoes_year, \n               aes(x = slon, y = slat, color = mag)) +\n    scale_color_distiller(palette = \"YlOrRd\", direction = 1, name = \"Magnitude*\") +\n    coord_fixed(1.3) +\n    labs(title = paste(\"Tornadoes in the US in\", year_input),\n         subtitle = \"Source: NOAA's National Weather Service Storm Prediction Center\",\n         caption = \"*Tornado magnitudes were measured on the Fujita scale.\") +\n    theme_void() +\n    theme(panel.background = element_rect(fill = \"azure2\",\n                                colour = \"black\",\n                                linewidth = 0.5))\n}\n\nplot_tornadoes_year(1950)\n\n\n\n\n\n\n\n\n\nCode\nplot_tornadoes_year(2022)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Group member 2 work here!\n#making an iteration with purrr::map\nlibrary(purrr)\n\n# Choosing the two years I want to map\nyears &lt;- c(1950, 2022)\n\n# Generate plots for each year and store them in a list\nplot_iterations &lt;- purrr::map(years, ~plot_tornadoes_year(.x))\n\nplot_iterations[]\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Group member 3 work here!--choropleth map\nplot_choropleth_tornadoes &lt;- function(year_input) {\n  tornadoes_year &lt;- tornadoes_data |&gt;\n    filter(yr == year_input) |&gt;\n    mutate(state_name = tolower(state.name[match(st, state.abb)]))\n\n  #calculating proportion by state\n  choropleth_data_new &lt;- tornadoes_year |&gt;\n    group_by(state_name) |&gt;\n    summarise(count = n(), .groups = \"drop\") |&gt;\n    mutate(proportion = count / nrow(tornadoes_year))\n\n  #joining with USA map data\n  map_data_joined &lt;- left_join(usa, choropleth_data_new, by = c(\"region\" = \"state_name\"))\n\n  #plotting\n  ggplot(map_data_joined, aes(x = long, y = lat, group = group, fill = proportion)) +\n    geom_polygon(color = \"black\") +\n     scale_fill_distiller(palette = \"YlOrRd\", direction = 1, name = \"Proportion of Tornadoes\", na.value = \"white\") +\n    coord_fixed(1.3) +\n    labs(title = paste(\"Proportion of Tornadoes by State in\", year_input),\n         subtitle = \"Source: NOAA's National Weather Service Storm Prediction Center\") +\n    theme_void() +\n    theme(panel.background = element_rect(fill = \"azure2\",\n                                colour = \"black\",\n                                linewidth = 0.5))\n}\n\nplot_choropleth_tornadoes(1950)\n\n\n\n\n\n\n\n\n\nCode\nplot_choropleth_tornadoes(2022)\n\n\n\n\n\n\n\n\n\n\n\nBlog Post\nWe analyzed the National Oceanic and Atmospheric Administration (NOAA) tornado dataset from Kaggle, which contains detailed information about tornadoes in the United States from 1950 to 2022. The dataset includes variables such as each tornado’s date, location in longitude and latitude, and magnitude. Between 1950 and 2022, global warming and overall atmospheric changes have affected tornado behavior. Thus, the question we sought to answer was “How have tornado ‘hotspots’ shifted from 1950 to 2022?”\nTo explore our question, we created visual maps of tornado locations for both 1950 and 2022, and compared them. We first created a map using a function to generate tornado plots for a given year, and used it to create maps for both years. We recreated these maps using a purrr::map iteration as well. Comparing the two maps, we observed an increase in both the magnitude of the tornadoes, and an overall increase in the number of tornadoes between 1950 and 2022. For our second map, we created a choropleth map function to visualize the proportion of tornadoes in each US state for both years. By comparing the two choropleth maps between 1950 and 2022, we found that most states showed an increase in tornado activity over time, as indicated by a higher proportion of tornadoes across the country in 2022 compared to 1950.\nOne major limitation of our analysis is that we only compared data from two individual years—1950 and 2022—rather than examining trends across the full time series. Analyzing just two snapshots may not capture the broader patterns or variability in tornado activity over time. For instance, we observed that Oklahoma and Kansas experienced fewer tornadoes in 2022 compared to 1950, which is unusual given the general trend of increasing tornado activity with time. This anomaly could reflect that 2022 was simply an atypical year for those regions, or it could signal that climate change is leading to more unpredictable or shifting tornado patterns. Additionally, we filtered out tornadoes that did not have recorded magnitude data, which could have excluded weaker tornadoes and slightly skewed our visualizations toward stronger events.\nAn ethical concern about the dataset is that it may have historical biases in how tornadoes were recorded over time. In earlier decades in the dataset, especially the 1950s and 1960s, tornado detection relied heavily on human observations and reports, meaning that many weaker or rural tornadoes likely went undocumented. In contrast, modern technology has dramatically improved tornado detection and reporting accuracy. As a result, the apparent increase or shifts in tornado patterns over time might partially reflect improvements in data collection rather than true changes in tornado behavior. This introduces a historical reporting bias that needs to be considered when interpreting trends, to avoid drawing misleading conclusions about the effects of climate change or regional risk."
  },
  {
    "objectID": "Project1_SDS192/scorecard_analysis.html",
    "href": "Project1_SDS192/scorecard_analysis.html",
    "title": "SDS 192 Project 1",
    "section": "",
    "text": "Code\nscorecard &lt;- sc_init() |&gt;\n  sc_year(2018) |&gt;                 #Note how we are looking at only 2021 data here!\n  sc_filter(stabbr == \"MA\") |&gt;     #Note how we are looking at only Massachusetts data here!\n  #The line below shows variable selection (there are lots of variables)\n  sc_select(unitid, instnm, city, highdeg, control, ugds, adm_rate, costt4_a, costt4_p, pcip27, pctfloan, admcon7, wdraw_orig_yr2_rt, cdr3) |&gt;\n  sc_get()\n\n\n\n\nCode\n#  Here's an example of how to recode the control variable\n\n# We are renaming the column control_text from control\nscorecard$control_text &lt;-\n  #The recode function does the work. It calls 1 \"Public\", 2 \" Private nonprofit\",etc. \n  recode(\n    scorecard$control, \n    \"1\" = \"Public\", \n    \"2\" = \"Private nonprofit\", \n    \"3\" = \"Private for-profit\",\n    .default = NA_character_\n  )\n\n\n\n\nCode\nscorecard$admcon7_text &lt;-\n  recode(\n    scorecard$admcon7, \n    \"1\" = \"Required\", \n    \"2\" = \"Recommended\", \n    \"3\" = \"Not Required or Recommended\",\n    \"4\" = \"No information\",\n    \"5\" = \"Considered but not required\",\n    .default = NA_character_\n  )\n\n\n\nPlot 1\n\n\nCode\n#Categorical Plot \nggplot(data = scorecard, \n       aes (x = highdeg,\n            y = costt4_a,\n            fill = control_text)) +\n  labs (title = \"Cost of Attendance vs. Highest Degree Awarded\",\n        subtitle = \"Observing correlation between cost of attendance of an institution & the highest degree awarded\",\n        caption = \"Data Source: U.S Department of Education College Scorecard Data\",\n        x = \"Highest Degree Awarded\",\n        y = \"Cost of Attendance\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set1\") +\n  geom_col() +\n  coord_flip() + \n  theme (\n    plot.title = element_text (face = \"bold\", size = 16),\n    plot.subtitle = element_text (size = 10, face = \"italic\"),\n    plot.caption = element_text (size = 10, face = \"italic\"),\n    axis.title.x = element_text(size = 8),\n    axis.title.y = element_text(size = 8)\n  )\n\n\nWarning: Removed 57 rows containing missing values or values outside the scale range\n(`geom_col()`).\n\n\n\n\n\n\n\n\n\n\n\nPlot 2\n\n\nCode\n# Create plot 2 here\n # Create plot 2 here\n ggplot( data = scorecard,\n         aes(y = adm_rate , \n             x = costt4_a,\n             color= control_text)\n         )+\n  geom_jitter( width = 5000, height = 0.02, alpha = 0.6)+\n  \n  scale_color_brewer(palette= \"Set1\")+\n  \n  labs ( title = \"Cost of Attendance Vs. Admission\",\n         subtitle = \"Exploring the relationship between cost and admission rate\",\n         caption = \"Data Source: U.S. Department of Education College Scorecard Data \",\n         y = \"Admission Rate\",\n         x = \"Cost of Attendance (Annual)\")+\n  \ntheme_minimal()+\ntheme (\n  plot.title = element_text (face = \"bold\", size = 16),\n  plot.subtitle = element_text (size = 10, face = \"italic\"),\n  plot.caption = element_text (size = 10, face = \"italic\"),\n   axis.title.x = element_text(size = 8),\n   axis.title.y = element_text(size = 8)\n  )\n\n\nWarning: Removed 81 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nPlot 3\n\n\nCode\n# Create plot 3 here\nggplot(\n  data = scorecard,\n mapping = aes(\n    y = pcip27,\n    x = costt4_a,\n    color = control_text\n  )\n) + \n  geom_jitter(height = 0.01, width = 0.3, alpha = 0.7) +\n  scale_color_brewer(palette = \"Set1\"\n  )+\n    labs(title = \"Mathematics and Statistics in Relation to Cost of Attendance\",\n       subtitle = \"How cost of attendance affects percentage of graduates with degrees in Math and Statistics\",\n       caption = \"U.S. Department of Education College Scorecard Data\",\n       x = \"Cost of Attendance\", \n       y = \"Percentage of Math and Stats Degrees Awarded\",\n       color = \"Control of Institution\") +\n    theme_minimal()+\ntheme (\n  plot.title = element_text (face = \"bold\", size = 16),\n  plot.subtitle = element_text (size = 10, face = \"italic\"),\n  plot.caption = element_text (size = 10, face = \"italic\"),\n   axis.title.x = element_text(size = 8),\n   axis.title.y = element_text(size = 8),\n)\n\n\nWarning: Removed 57 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nBlog Post\nThe data used in this visualization comes from the U.S. Department of Education’s College Scorecard, a government website that gives information about colleges and universities across the United States. The data set is compiled from federal reporting requirements, including data submitted by institutions and other government sources. It includes numerical variables such as the cost of attendance, admission rates, and withdrawal rates and categorical variables such as highest degree offered and control_text helping students and families make informed decisions about higher education. In our analysis, we will visualize the relationship between the cost of attendance and other factors, specifically examining whether there is a correlation between cost and admission rates or cost and field of study. By exploring these relationships, we aim to uncover patterns that may help prospective students understand how pricing relates to selectivity and academic programs.\nOur question centers “How does cost of attendance affect various factors related to higher education in Massachusetts?” In our first plot we tried to find out how highest degree awarded was affected by the cost of attendance for an institution in Mass. Colleges that award graduate degrees have the highest cost of attendance, bachelor’s the second highest, associate’s the third highest, whereas colleges that only award certificate degrees have the lowest cost of attendance. This indicates that the higher degree a college awards, the more expensive it will be on average. Our second plot asked the question, “How is admission rate affected by cost of attendance?” This data is a bit more ambiguous, but it does show a general trend of lower admission rate corresponding to a higher cost of attendance. In other words, the plot shoes that the more selective a university is, it is more likely to have a higher cost of attendance. Our final plot explores the relationship between mathematics and statistics graduates, and an institution’s cost of attendance. This plot also incorporates the control of the institution. The percentage of students graduating with math and statistics degrees dramatically increases with the institution’s cost of attendance. This shows that schools that are more expensive tend to graduate more math and stats degrees than less expensive institution. The control of the institution is also relevant to this data, as it shows that public universities cost much less, and therefore graduate fewer math and stats majors.\nThe key takeaway from our data analysis is that higher cost of attendance for universities have lower admission rates, higher number of math/stats degrees awarded, and higher degree levels obtained by students. These conclusions could indicate that universities with a higher cost of attendance offer more education resources leading to higher degree levels obtained, higher education quality leading to increased selectivity (and therefore lower admission rates), and stronger programs and resources for STEM leading to higher numbers of math and stats major graduates, especially in private institutions. An ethical concern to consider is the bias in the data collection and responses. Since the observations do not highlight each individuals’ socioeconomic background, there could be other factors besides cost of attendance that influenced these results. For example, higher cost of attendance tends to represent private institutions, and these schools may have higher math/stats degree students due to lower financial barriers, rather than the school offering better education support. These potential biases could lead to a misinterpretation of all students attending these institutions who come from diverse socioeconomic backgrounds."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hello! My name is Ayano Tamura, and I am a second year student at Smith College, double majoring in Economics and Statistical & Data Science, working towards a concentration in Global Finance. I’m passionate about using data to understand real-world issues and drive meaningful insights.\nI’m interested in using data analysis as a tool to come up with business strategies and solutions in the Marketing, Financial, and Sports industry. Some of my current career interests include Marketing, Marketing Research, Consulting, and Sports Analytics. I have technical proficiency in Python, Java, R, as well as website development tools (HTML, CSS, JavaScript).\nOn campus, I’m involved in a wide range of organizations, from Student Government, to Japanese Student Association, and Gold Key (Tour Guide). My current role as Class President has especially helped me build my leadership and team management skills. I love working with my cabinet members to plan and execute events and spaces for community engagement.\nMy hobbies include traveling, dancing, and making Spotify playlists. If any of my skills, experiences, or hobbies interest you, please reach out! I would love to connect with you.\n  Connect with me on LinkedIn \n  Email me"
  },
  {
    "objectID": "Project2_SDS192/WCA_data/WAC_README.html",
    "href": "Project2_SDS192/WCA_data/WAC_README.html",
    "title": "World Cube Association – Results Database Export",
    "section": "",
    "text": "Encoding: UTF-8\nDate: March 12, 2025\nExport Format Version: 1.0.0\nContact: WCA Results Team (https://www.worldcubeassociation.org/contact?contactRecipient=wrt)\nWebsite: https://www.worldcubeassociation.org/export/results\n\n\n\nThis database export contains public information about all official WCA competitions, WCA members, and WCA competition results.\n\n\n\nThe goal of this database export is to provide members of the speedcubing community a practical way to perform analysis on competition information for statistical and personal purposes.\n\n\n\nThe information in this file may be re-published, in whole or in part, as long as users are clearly notified of the following:\n\nThis information is based on competition results owned and maintained by the World Cube Assocation, published at https://worldcubeassociation.org/results as of March 12, 2025.\n\n\n\n\nThe WCA database was originally created and maintained by:\n\nClément Gallet, France\nStefan Pochmann, Germany\nJosef Jelinek, Czech Republic\nRon van Bruchem, Netherlands\n\nThe database contents are now maintained by the WCA Results Team, and the software for the database is maintained by the WCA Software Team: https://www.worldcubeassociation.org/about\n\n\n\nThe export contains a metadata.json file, with the following fields:\n\n\n\nField\nSample Value\n\n\n\n\nexport_date\n\"2025-03-12 00:02:02 UTC\"\n\n\nexport_format_version\n\"1.0.0\"\n\n\n\nIf you regularly process this export, we recommend that you check the export_format_version value in your program and and review your code if the major part of the version (the part before the first .) changes.\nIf you are processing the exported data using an automated system, we recommend using a cron job to check the API endpoint at: https://www.worldcubeassociation.org/api/v0/export/public You can use the export_date to detect if there is a new export, and the sql_url and tsv_url will contain the URLs for the corresponding downloads.\n\n\n\nThe database export consists of these tables:\n\n\n\n\n\n\n\nTable\nContents\n\n\n\n\nPersons\nWCA competitors\n\n\nCompetitions\nWCA competitions\n\n\nEvents\nWCA events (3x3x3 Cube, Megaminx, etc)\n\n\nResults\nWCA results per competition+event+round+person\n\n\nRanksSingle\nBest single result per competitor+event and ranks\n\n\nRanksAverage\nBest average result per competitor+event and ranks\n\n\nRoundTypes\nThe round types (first, final, etc)\n\n\nFormats\nThe round formats (best of 3, average of 5, etc)\n\n\nCountries\nCountries\n\n\nContinents\nContinents\n\n\nScrambles\nScrambles\n\n\nchampionships\nChampionship competitions\n\n\neligible_country_iso2s_for_championship\nSee explanation below\n\n\n\nMost of the tables should be self-explanatory, but here are a few specific details:\n\n\nCountries stores include those from the Wikipedia list of countries at http://en.wikipedia.org/wiki/List_of_countries, and may include some countries that no longer exist. The ISO2 column should reflect ISO 3166-1 alpha-2 country codes, for countries that have them. Custom codes may be used in some circumstances.\n\n\n\nScrambles stores all scrambles.\nFor 333mbf, an attempt is comprised of multiple newline-separated scrambles. However, newlines can cause compatibility issues with TSV parsers. Therefore, in the TSV version of the data we replace each newline in a 333mbf scramble with the | character.\n\n\n\neligible_country_iso2s_for_championship stores information about which citizenships are eligible to win special cross-country championship types.\nFor example, greater_china is a special championship type which contains 4 iso2 values: CN, HK, MC and TW. This means that any competitor from China, Hong Kong, Macau, or Taiwan is eligible to win a competition with championship type greater_china.\n\n\n\nPlease see https://www.worldcubeassociation.org/regulations/#article-9-events for information about how results are measured.\nValues of the Results table can be interpreted as follows:\n\nThe result values are in the following fields value1, value2, value3, value4, value5, best, and average.\nThe value -1 means DNF (Did Not Finish).\nThe value -2 means DNS (Did Not Start).\nThe value 0 means “no result”. For example a result in a best-of-3 round has a value of 0 for the value4, value5, and average fields.\nPositive values depend on the event; see the column “format” in Events.\n\nMost events have the format “time”, where the value represents centiseconds. For example, 8653 means 1 minute and 26.53 seconds.\nThe format “number” means the value is a raw number, currently only used by “fewest moves” for number of moves.\n\nFewest moves averages are stored as 100 times the average, rounded.\n\nThe format “multi” is for old and new multi-blind, encoding the time as well as the number of cubes attempted and solved. This is a decimal value, which can be interpreted (“decoded”) as follows:\nold: 1SSAATTTTT\n     solved        = 99 - SS\n     attempted     = AA\n     timeInSeconds = TTTTT (99999 means unknown)\nnew: 0DDTTTTTMM\n     difference    = 99 - DD\n     timeInSeconds = TTTTT (99999 means unknown)\n     missed        = MM\n     solved        = difference + missed\n     attempted     = solved + missed\nIn order to encode data, use the following procedure:\n     solved        = # cubes solved\n     attempted     = # cubes attempted\n     missed        = # cubes missed = attempted - solved\n     DD            = 99 - (solved - missed)\n     TTTTT         = solve time in seconds\n     MM            = missed\nNote that this is designed so that a smaller decimal value means a better result. This format does not support more than 99 attempted cubes, or times greater than 99999 seconds (about 27.7 hours)."
  },
  {
    "objectID": "Project2_SDS192/WCA_data/WAC_README.html#description",
    "href": "Project2_SDS192/WCA_data/WAC_README.html#description",
    "title": "World Cube Association – Results Database Export",
    "section": "",
    "text": "This database export contains public information about all official WCA competitions, WCA members, and WCA competition results."
  },
  {
    "objectID": "Project2_SDS192/WCA_data/WAC_README.html#goal",
    "href": "Project2_SDS192/WCA_data/WAC_README.html#goal",
    "title": "World Cube Association – Results Database Export",
    "section": "",
    "text": "The goal of this database export is to provide members of the speedcubing community a practical way to perform analysis on competition information for statistical and personal purposes."
  },
  {
    "objectID": "Project2_SDS192/WCA_data/WAC_README.html#allowed-use",
    "href": "Project2_SDS192/WCA_data/WAC_README.html#allowed-use",
    "title": "World Cube Association – Results Database Export",
    "section": "",
    "text": "The information in this file may be re-published, in whole or in part, as long as users are clearly notified of the following:\n\nThis information is based on competition results owned and maintained by the World Cube Assocation, published at https://worldcubeassociation.org/results as of March 12, 2025."
  },
  {
    "objectID": "Project2_SDS192/WCA_data/WAC_README.html#acknowledgements",
    "href": "Project2_SDS192/WCA_data/WAC_README.html#acknowledgements",
    "title": "World Cube Association – Results Database Export",
    "section": "",
    "text": "The WCA database was originally created and maintained by:\n\nClément Gallet, France\nStefan Pochmann, Germany\nJosef Jelinek, Czech Republic\nRon van Bruchem, Netherlands\n\nThe database contents are now maintained by the WCA Results Team, and the software for the database is maintained by the WCA Software Team: https://www.worldcubeassociation.org/about"
  },
  {
    "objectID": "Project2_SDS192/WCA_data/WAC_README.html#date-and-format-version",
    "href": "Project2_SDS192/WCA_data/WAC_README.html#date-and-format-version",
    "title": "World Cube Association – Results Database Export",
    "section": "",
    "text": "The export contains a metadata.json file, with the following fields:\n\n\n\nField\nSample Value\n\n\n\n\nexport_date\n\"2025-03-12 00:02:02 UTC\"\n\n\nexport_format_version\n\"1.0.0\"\n\n\n\nIf you regularly process this export, we recommend that you check the export_format_version value in your program and and review your code if the major part of the version (the part before the first .) changes.\nIf you are processing the exported data using an automated system, we recommend using a cron job to check the API endpoint at: https://www.worldcubeassociation.org/api/v0/export/public You can use the export_date to detect if there is a new export, and the sql_url and tsv_url will contain the URLs for the corresponding downloads."
  },
  {
    "objectID": "Project2_SDS192/WCA_data/WAC_README.html#format-version-1.0.0",
    "href": "Project2_SDS192/WCA_data/WAC_README.html#format-version-1.0.0",
    "title": "World Cube Association – Results Database Export",
    "section": "",
    "text": "The database export consists of these tables:\n\n\n\n\n\n\n\nTable\nContents\n\n\n\n\nPersons\nWCA competitors\n\n\nCompetitions\nWCA competitions\n\n\nEvents\nWCA events (3x3x3 Cube, Megaminx, etc)\n\n\nResults\nWCA results per competition+event+round+person\n\n\nRanksSingle\nBest single result per competitor+event and ranks\n\n\nRanksAverage\nBest average result per competitor+event and ranks\n\n\nRoundTypes\nThe round types (first, final, etc)\n\n\nFormats\nThe round formats (best of 3, average of 5, etc)\n\n\nCountries\nCountries\n\n\nContinents\nContinents\n\n\nScrambles\nScrambles\n\n\nchampionships\nChampionship competitions\n\n\neligible_country_iso2s_for_championship\nSee explanation below\n\n\n\nMost of the tables should be self-explanatory, but here are a few specific details:\n\n\nCountries stores include those from the Wikipedia list of countries at http://en.wikipedia.org/wiki/List_of_countries, and may include some countries that no longer exist. The ISO2 column should reflect ISO 3166-1 alpha-2 country codes, for countries that have them. Custom codes may be used in some circumstances.\n\n\n\nScrambles stores all scrambles.\nFor 333mbf, an attempt is comprised of multiple newline-separated scrambles. However, newlines can cause compatibility issues with TSV parsers. Therefore, in the TSV version of the data we replace each newline in a 333mbf scramble with the | character.\n\n\n\neligible_country_iso2s_for_championship stores information about which citizenships are eligible to win special cross-country championship types.\nFor example, greater_china is a special championship type which contains 4 iso2 values: CN, HK, MC and TW. This means that any competitor from China, Hong Kong, Macau, or Taiwan is eligible to win a competition with championship type greater_china.\n\n\n\nPlease see https://www.worldcubeassociation.org/regulations/#article-9-events for information about how results are measured.\nValues of the Results table can be interpreted as follows:\n\nThe result values are in the following fields value1, value2, value3, value4, value5, best, and average.\nThe value -1 means DNF (Did Not Finish).\nThe value -2 means DNS (Did Not Start).\nThe value 0 means “no result”. For example a result in a best-of-3 round has a value of 0 for the value4, value5, and average fields.\nPositive values depend on the event; see the column “format” in Events.\n\nMost events have the format “time”, where the value represents centiseconds. For example, 8653 means 1 minute and 26.53 seconds.\nThe format “number” means the value is a raw number, currently only used by “fewest moves” for number of moves.\n\nFewest moves averages are stored as 100 times the average, rounded.\n\nThe format “multi” is for old and new multi-blind, encoding the time as well as the number of cubes attempted and solved. This is a decimal value, which can be interpreted (“decoded”) as follows:\nold: 1SSAATTTTT\n     solved        = 99 - SS\n     attempted     = AA\n     timeInSeconds = TTTTT (99999 means unknown)\nnew: 0DDTTTTTMM\n     difference    = 99 - DD\n     timeInSeconds = TTTTT (99999 means unknown)\n     missed        = MM\n     solved        = difference + missed\n     attempted     = solved + missed\nIn order to encode data, use the following procedure:\n     solved        = # cubes solved\n     attempted     = # cubes attempted\n     missed        = # cubes missed = attempted - solved\n     DD            = 99 - (solved - missed)\n     TTTTT         = solve time in seconds\n     MM            = missed\nNote that this is designed so that a smaller decimal value means a better result. This format does not support more than 99 attempted cubes, or times greater than 99999 seconds (about 27.7 hours)."
  }
]